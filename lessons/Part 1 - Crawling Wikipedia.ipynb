{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Crawling Wikipedia\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We're going to create a social network of characters in the Marvel Cinematic Universe. You are looking at a Jupyter notebook. Each section is a cell that can contain text or Python code. You can run a cell by selecting it, and hitting `Ctrl-Enter`. You will see the results of your code as it runs. Try running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import wikinetworking as wn\n",
    "import networkx as nx\n",
    "from pyquery import PyQuery\n",
    "%matplotlib inline\n",
    "\n",
    "print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just ran some Python code that imports _*packages*_. Packages are pre-written Python code. The `wikinetworking` package contains code for crawling, text mining and graphing Wiki articles. You can access these functions in the `wn` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of links\n",
    "\n",
    "Our first step is getting a list of links that we want to crawl. [Wikipedia](https://en.wikipedia.org) has article data organized as [lists of topics](https://en.wikipedia.org/wiki/Portal%3AContents%2FLists). (There are many that may not be listed on this page. You should search for one that works for you.) Once you find the URL of a list that contains articles you would like to crawl, paste the URL in the variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_film_actors\"\n",
    "print url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download the article and get a list of links from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = wn.filter_links(PyQuery(url=url))\n",
    "print links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these links may not be relevant to our topic. We can filter for links that exist inside certain types of HTML elements. You can find the type of element by inspecting a relevant link on your Wikipedia page in your browser. We can use a special type of filter called a [CSS selector](https://www.w3schools.com/cssref/css_selectors.asp) to get only links that are inside of specific types of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector=\"th\"\n",
    "\n",
    "links = wn.filter_links(PyQuery(url=url), selector=selector)\n",
    "print links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross referencing lists of links\n",
    "\n",
    "Another way of filtering links is to try to cross-reference them with another list of links. First, find another URL that might shares the links you want, but excludes the links you don't want from the first list of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "another_url = \"\"\n",
    "another_selector = \"\"\n",
    "more_links = wn.filter_links(PyQuery(url=another_url), selector=another_selector)\n",
    "print more_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you need links from a [lists of lists](https://en.wikipedia.org/wiki/List_of_lists_of_lists)? You can automatically crawl a list of URLs, as well. First we need to generate a list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_pattern = \"https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_\"\n",
    "\n",
    "sections = [letter for letter in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ']\n",
    "sections.append('0-9')\n",
    "\n",
    "many_urls = [url_pattern + section for section in sections]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can crawl this list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector = \".hatnote\"\n",
    "\n",
    "more_links = wn.retrieve_multipage(many_urls, selector=selector, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a second set of links, we can look for the intersection of the two lists. That should give us only the URLs we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_links = wn.intersection(links, more_links)\n",
    "print relevant_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these links into a file so we don't have to download the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wn.write_list(relevant_links, \"relevant_links.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make sure we can load the data after we've saved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_links = wn.read_list(\"relevant_links.txt\")\n",
    "print relevant_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling The Links\n",
    "\n",
    "Now that we have the `relevant_links` list, we just need to choose a starting article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starting_url=\"\"\n",
    "\n",
    "raw_crawl_data = wn.crawl(starting_url, accept=relevant_links)\n",
    "\n",
    "import json\n",
    "print json.dumps(raw_crawl_data, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"flatten\" the data and save it for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_data = wn.undirected_graph(raw_crawl_data)\n",
    "\n",
    "wn.save_dict(graph_data, \"undirected_graph.json\")\n",
    "print load_dict(\"undirected_graph.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next...\n",
    "\n",
    "Now we can draw the graph. Go to [Part 2 - Drawing the Network](./Part%202%20-%20Drawing%20the%20Network.ipynb)..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
