{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling Lesson Hints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marvel Cinematic Universe\n",
    "\n",
    "##### First list\n",
    "[List of all MCU Actors](https://en.wikipedia.org/wiki/List_of_Marvel_Cinematic_Universe_film_actors)\n",
    "\n",
    "CSS selector: `th`\n",
    "\n",
    "This contains links to both characters and actors\n",
    "\n",
    "##### Second list\n",
    "[List of all Marvel Comics characters](https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters)\n",
    "\n",
    "CSS selector: `.hatnote`\n",
    "\n",
    "This is a multi-page list of lists, where each article URL begins with  `https://en.wikipedia.org/wiki/List_of_Marvel_Comics_characters:_` and ends with any of the values from ['ABCDEFGHIJKLMNOPQRSTUVWXYZ'] and '0-9'. Therefore, we must construct a list of URLs of each individual list and do a multi-page retrieval.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "This is an extremely dense graph, due to the number of links between any two articles. An undirected graph (which sums links back and forth between two articles as a single weighted edge) with a minimum weight greater than 1 helps cut down on clutter. In addition, a `spring_layout` creates some interesting groupings of individual articles.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`mcu_network.json`](./mcu_network.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BET Hip Hop Award Winners\n",
    "\n",
    "##### First list: \n",
    "\n",
    "[List of all Hip Hop musicians](https://en.wikipedia.org/wiki/List_of_hip_hop_musicians)\n",
    "\n",
    "CSS selector: `li`\n",
    "\n",
    "This contains links to all Hip Hop musicians with a Wiki article.\n",
    "\n",
    "##### Second list:\n",
    "\n",
    "[List of all BET Hip Hop awards](https://en.wikipedia.org/wiki/BET_Hip_Hop_Awards)\n",
    "\n",
    "CSS selector: `li`\n",
    "\n",
    "This contains links to all BET Award Winner musicians and the names of the works for which they won an award.\n",
    "\n",
    "##### Crawling notes\n",
    "\n",
    "Crawling and saving the graph as an undirected graph and as a directed graph can generate extremely different visualizations.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "When the graph is flattened as a directed graph and then visualized as a directed graph with minimum weight of 2, a number of artists are forced to outside of the graph in a ring shape.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`bet_directed.json`](./bet_directed.json) and [`bet_undirected.json`](./bet_undirected.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA All Stars\n",
    "\n",
    "[List of NBA All-Stars](https://en.wikipedia.org/wiki/List_of_NBA_All-Stars)\n",
    "\n",
    "CSS selector: `.fn`\n",
    "\n",
    "This contains links to NBA All-Stars.\n",
    "\n",
    "##### Crawling notes\n",
    "\n",
    "There are 408 All-stars, so you must set `max_articles=408`. In addition, articles about basketball players often contain dozens of links to other players that may not be relevant for building a social network. Therefore, setting `selector=\"p\"` as a crawl setting limits relevant links to only links that appear in paragraph text.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "When creating the graph, setting `minimum_weight=2` seems to cluster basketball players together that were active during the same decades.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`nba_allstars.json`](./nba_allstars.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFL Players\n",
    "\n",
    "[List of NFL Players by number of games played](https://en.wikipedia.org/wiki/List_of_NFL_players_by_games_played)\n",
    "\n",
    "CSS selector: `.fn`\n",
    "\n",
    "This contains links to the approximately 300 NFL players who have played the most games.\n",
    "\n",
    "##### Crawling notes\n",
    "\n",
    "There are 300 players, so you must set `max_articles=300`. Setting `selector=\"p\"` as a crawl setting limits relevant links to only links that appear in paragraph text.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "The graph is very sparse with few connections between players. Setting `minimum_weight=2` creates an interesting cluster of career quarterbacks at the center of the graph.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`nfl_players.json`](./nfl_players.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwatch Characters\n",
    "\n",
    "The [Overwatch Wiki](http://overwatch.wikia.com/wiki/Overwatch_Wiki) has some special requirements for crawling. Because of the structure of the [Heroes page](http://overwatch.wikia.com/wiki/Heroes) does not have any convenient CSS selectors for hero links, it is easier to hand code the URLs for all 24 heroes.\n",
    "\n",
    "```\n",
    "# Note: Lucio and Torbjorn have special characters in their names. You can inspect their URLs for their names\n",
    "heroes = [ \"Genji\", \"McCree\", \"Pharah\", \"Reaper\", \"Soldier:_76\", \"Sombra\", \"Tracer\",\n",
    "         \"Bastion\", \"Hanzo\", \"Junkrat\", \"Mei\", \"Torbj%C3%B6rn\", \"Widowmaker\", \n",
    "         \"D.Va\", \"Orisa\", \"Reinhardt\", \"Roadhog\", \"Winston\", \"Zarya\",\n",
    "         \"Ana\", \"L%C3%BAcio\", \"Mercy\", \"Symmetra\", \"Zenyatta\"]\n",
    "urls = [ \"/wiki/\" + hero for hero in heroes ]\n",
    "```\n",
    "\n",
    "##### Crawling notes\n",
    "\n",
    "The Overwatch Wiki is not a Wikipedia wiki. Therefore, we have to change a few of the crawling options. Specifically, you must set `host=\"https://overwatch.wikia.com\", title_selector=\"h1\"`. In addition, you should also set ` selector=\"p\"` due to the fact that each article actually contains a link to every other hero.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "The Overwatch Wiki has very sparse information about each hero. You may set `minimum_weight=1`. The groupings will not be strong, nor will the node sizes show a lot of variation. However, you can see individuals whose narratives are tied to each other. For example, members of [Talon](http://overwatch.wikia.com/wiki/Talon) may appear close to each other.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`overwatch.json`](./overwatch.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forbes 400\n",
    "\n",
    "\n",
    "##### Crawling notes\n",
    "The [Forbes 400 Wikipedia](https://en.wikipedia.org/wiki/List_of_members_of_the_Forbes_400) entry is extremely incomplete. To create the crawl list, you can text mine the [Forbes 400](https://www.forbes.com/sites/chasewithorn/2016/10/04/forbes-400-the-full-list-of-the-richest-people-in-america-2016/#3c2f82d422f4) list directly. Each entry is contained inside of a `strong` tag and matches a pattern that begins with a number, followed by a period. To convert this data into a useable list of Wikipedia articles, use the following code:\n",
    "\n",
    "```\n",
    "from pyquery import PyQuery\n",
    "\n",
    "# Make a list of URLs to mine\n",
    "urls = [ \"https://www.forbes.com/sites/chasewithorn/2016/10/04/forbes-400-the-full-list-of-the-richest-people-in-america-2016/#3381da7c22f4\",\n",
    "         \"https://www.forbes.com/sites/chasewithorn/2016/10/04/forbes-400-the-full-list-of-the-richest-people-in-america-2016/2/#1bf172cb7b17\", \n",
    "         \"https://www.forbes.com/sites/chasewithorn/2016/10/04/forbes-400-the-full-list-of-the-richest-people-in-america-2016/3/#5722e9247c58\", \n",
    "         \"https://www.forbes.com/sites/chasewithorn/2016/10/04/forbes-400-the-full-list-of-the-richest-people-in-america-2016/4/#3262ecd31473\"]\n",
    "\n",
    "strongs = list()\n",
    "\n",
    "# Get each \"strong\" HTML tag from each url\n",
    "for url in urls:\n",
    "    strongs.extend(PyQuery(url=url)(\"strong\"))\n",
    "\n",
    "# Use regular expressions to do the heavy lifting\n",
    "import re\n",
    "\n",
    "# This regex matches any number of digits followed by a period and a space, then accepts the rest of the string\n",
    "# For a full explanation of this regex, see https://regex101.com/r/L2ZNig/2\n",
    "regex = re.compile(\"^\\d+\\. .+\")\n",
    "\n",
    "# Use another regex to delete the list number, replace any spaces with an underscore\n",
    "forbes_400 = [ \"/wiki/\" + re.sub(\"^\\d+\\. \", \"\", strong.text).replace(\" \", \"_\") \\\n",
    "                for strong in strongs if strong.text and regex.match(strong.text) ]\n",
    "\n",
    "print forbes_400\n",
    "```\n",
    "\n",
    "You may then perform the crawl with `max_articles=400`.\n",
    "\n",
    "_*Note*_: Not all entries will be real Wikipedia entries. This code may generate some error messages during the crawl.\n",
    "\n",
    "##### Visualization notes\n",
    "\n",
    "When creating the graph, a `minimum_weight=3` will create clusters of individuals with business or family ties to each other.\n",
    "\n",
    "##### Data files\n",
    "\n",
    "[`forbes_400.json`](./forbes_400.json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
